{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ae05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4ed5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn comes with a number of toy datasets (https://sklearn.org/datasets/index.html#toy-datasets)\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the wine dataset from sklearn. You may want to take a look at the format of the dataset\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# Save the datapoints into the variable X and the targets into the variable y\n",
    "X = wine.data\n",
    "y = wine.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e467f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the function train_test_split from sklearn and use this to split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The function returns splits of each array passed in. \n",
    "# The proportion to be used as the training set is given by test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# We first import the classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# We instantiate the classifier with 5 neighbours\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# We fit the model using our training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Finally, we generate predictions on the test data\n",
    "ypred_test=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec5cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data#\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#the target test set = y_test and predicted values = ypred_test\n",
    "cm = confusion_matrix(y_test, ypred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32461ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  1  0]\n",
      " [ 1 10  3]\n",
      " [ 1  4  7]]\n"
     ]
    }
   ],
   "source": [
    "#show matrix#\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "043d8c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'row=Predict  \\ncolumn=True   \\nIf A is what we want   \\n\\n    A     B   \\nA   TP    FN   \\nB   FP    TN   \\n\\nps:   \\nTP=True Positive   \\nTN=True Negative  \\nFP=False Positive   \\nFN=False Negative'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''row=Predict  \n",
    "column=True   \n",
    "If A is what we want   \n",
    "\n",
    "    A     B   \n",
    "A   TP    FN   \n",
    "B   FP    TN   \n",
    "\n",
    "ps:   \n",
    "TP=True Positive   \n",
    "TN=True Negative  \n",
    "FP=False Positive   \n",
    "FN=False Negative'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ae392e1",
   "metadata": {},
   "source": [
    "Accuracy = All TP/sum of matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facb37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy(y, pred):\n",
    "    cm = confusion_matrix(y, pred)\n",
    "    acc = np.diagonal(cm).sum()/cm.sum()\n",
    "    return acc\n",
    "\n",
    "### np.diagonal() 矩阵斜对角线， .sum就是求和###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc571607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Recall_i=True Positives/(TP+FN)   \\nRecall=1/k*sum of recall_i'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Recall_i=True Positives/(TP+FN)   \n",
    "Recall=1/k*sum of recall_i'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ca1ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_recall_macro(y, pred):\n",
    "    recalls = []\n",
    "    cm = confusion_matrix(y, pred)\n",
    "    TP = np.diagonal(cm)\n",
    "    raw_sum = cm.sum(axis=1) #每一行求和#\n",
    "    \n",
    "    for i in range(len(cm)):\n",
    "        recall_i = TP[i]/raw_sum[i]\n",
    "        recalls.append(recall_i)\n",
    "\n",
    "    return np.mean(recalls) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63e6152e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Precision_1=TP/(TP+FP)     \\nPrecision=1/k*sum of precision_i'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Precision_1=TP/(TP+FP)     \n",
    "Precision=1/k*sum of precision_i'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78d19652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_precision_macro(y, pred):\n",
    "    precs = [] \n",
    "    cm = confusion_matrix(y, pred)\n",
    "    col_sum = cm.sum(axis=0)\n",
    "    TP = np.diagonal(cm)\n",
    "    \n",
    "    for i in range(len(cm)):\n",
    "        precs_i = TP[i]/col_sum[i]\n",
    "        precs.append(precs_i)\n",
    "    \n",
    "\n",
    "    return np.mean(precs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592eb92",
   "metadata": {},
   "source": [
    "Check that whether the functions has match those in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58530fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "my_accuracy(y_test, ypred_test) == accuracy_score(y_test, ypred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96e5b743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_recall_macro(y_test, ypred_test)==recall_score(y_test, ypred_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "359deb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_precision_macro(y_test, ypred_test)==precision_score(y_test, ypred_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24b26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67c773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
